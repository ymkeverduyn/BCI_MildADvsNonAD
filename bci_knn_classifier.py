# -*- coding: utf-8 -*-
"""Kopie von BCIKNN

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NSYy80BXsx81sccL_lQqLE5ztA6vg3oc
"""

# Commented out IPython magic to ensure Python compatibility.
#SETUP

COLAB = True
SKTIME_INSTALLED = False

if COLAB:
    from google.colab import drive
    drive.mount('/content/drive')
    # Load the contents of the directory
    !ls
    # Change your working directory to the folder where you stored your files, e.g.
#     %cd /content/drive/MyDrive/BCI project

#Load needed modules
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV, cross_validate, LeaveOneOut, KFold
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score
from sklearn import preprocessing

#Load data
Auditory = pd.read_csv('Kopie van merged_erp_features.csv')
display(Auditory)

Resting = pd.read_csv('Resting State Data /resting_state_complete.csv')
display(Resting)

# exclude participant 9 from ERP data for better comparison
Auditory = Auditory.drop(Auditory[Auditory.Participant == 'sub-009'].index).reset_index(drop=True)

# display participants 5 and 8 data for comparison
display(Auditory[Auditory['Participant'] == 'sub-005'])
display(Auditory[Auditory['Participant'] == 'sub-008'])

display(Resting[Resting['Participant'] == 'participant005'])
display(Resting[Resting['Participant'] == 'participant008'])

# change the structure of the dataframes so that each participant's data is in either test or train, not both
# and there are both classes in all folds for CV

# Identify the row indices for participants five and eight
participant_5_indices_auditory = Auditory[Auditory['Participant'] == 'sub-005'].index
participant_8_indices_auditory = Auditory[Auditory['Participant'] == 'sub-008'].index

participant_5_indices_rest = Resting[Resting['Participant'] == 'participant005'].index
participant_8_indices_rest = Resting[Resting['Participant'] == 'participant008'].index

# Swap the rows for participants five and eight for all columns in Auditory
for column in Auditory.columns:
    Auditory.loc[participant_5_indices_auditory, column], Auditory.loc[participant_8_indices_auditory, column] = Auditory.loc[participant_8_indices_auditory, column].values, Auditory.loc[participant_5_indices_auditory, column].values

Auditory = Auditory.reset_index(drop=True)

# Swap the rows for participants five and eight for all columns in Resting
for column in Resting.columns:
    Resting.loc[participant_5_indices_rest, column], Resting.loc[participant_8_indices_rest, column] = Resting.loc[participant_8_indices_rest, column].values, Resting.loc[participant_5_indices_rest, column].values


Resting = Resting.reset_index(drop=True)

# display participants 5 and 8 data for comparison
display(Auditory[Auditory['Participant'] == 'sub-005'])
display(Auditory[Auditory['Participant'] == 'sub-008'])

display(Resting[Resting['Participant'] == 'participant005'])
display(Resting[Resting['Participant'] == 'participant008'])

# Convert the 'Condition' column to numeric
Auditory['Condition'] = Auditory['Condition'].map({'MCI': 1, 'NonAD': 0})

# Verify the conversion
print(Auditory['Condition'])

# Convert 'Participant' column to numeric 'label' column
Resting['Participant'] = Resting['Participant'].map({'participant001': 0, 'participant002': 1, 'participant003': 0, 'participant004': 1, 'participant005': 1,
                                                           'participant007': 1, 'participant008': 0, 'participant010': 0, 'participant011': 1, 'participant012': 0,})

Resting = Resting.rename(columns={'Participant' : 'label'})

print(Resting.label.value_counts())
display(Resting)

# Define the target variable Y and features X
Y_Auditory = Auditory['Condition']
X_Auditory = Auditory.drop(columns=['Epoch', 'Channel', 'Condition', 'Participant'])

auditory_cols = X_Auditory.columns

# scale features
x_auditoy = X_Auditory.values
min_max_scaler = preprocessing.MinMaxScaler()
x_auditoy_scaled = min_max_scaler.fit_transform(x_auditoy)
X_Auditory = pd.DataFrame(x_auditoy_scaled, columns=auditory_cols)

print(X_Auditory.shape)
print(Y_Auditory.shape)

Y_Rest = Resting['label']
unnamed_first_column = Resting.columns[0]
X_Rest = Resting.drop(columns=[unnamed_first_column, 'label'])

resting_cols = X_Rest.columns

# scale features
x_rest = X_Rest.values
min_max_scaler = preprocessing.MinMaxScaler()
x_rest_scaled = min_max_scaler.fit_transform(x_rest)
X_Rest = pd.DataFrame(x_rest_scaled, columns=resting_cols)

print(X_Rest.shape)
print(Y_Rest.shape)

display(X_Auditory)
display(X_Rest)

#Auditory KNN
# Define the range of k values to test
param_grid = {'n_neighbors': np.arange(1, 21)}

# Define the scoring metrics with zero_division parameter
scoring = {
    'accuracy': make_scorer(accuracy_score),
    'precision': make_scorer(precision_score, zero_division=0),
    'recall': make_scorer(recall_score, zero_division=0),
    'f1': make_scorer(f1_score, zero_division=0)
}

# Create the KNN classifier
knn = KNeighborsClassifier()

# Set up GridSearchCV with 5-fold cross-validation
cv = KFold(n_splits=5, shuffle=False)
grid_search = GridSearchCV(knn, param_grid, cv=cv, scoring=scoring, refit='accuracy', return_train_score=True)

# Perform the grid search on the auditory data
grid_search.fit(X_Auditory, Y_Auditory)

# Get the best parameters and the corresponding scores
best_k = grid_search.best_params_['n_neighbors']
best_score = grid_search.best_score_

print(f"Best k: {best_k}")
print(f"Best cross-validated accuracy: {best_score}")

# Extract the scores for the best model
best_model = grid_search.best_estimator_
scoresAuditory = cross_validate(best_model, X_Auditory, Y_Auditory, cv=cv, scoring=scoring, return_train_score=True)

print("Metrics Auditory with optimized k:")
print('Test Accuracy Auditory:', scoresAuditory['test_accuracy'], 'Mean:', scoresAuditory['test_accuracy'].mean())
print('Train Accuracy Auditory:', scoresAuditory['train_accuracy'], 'Mean:', scoresAuditory['train_accuracy'].mean())
print('Test Precision Auditory:', scoresAuditory['test_precision'], 'Mean:', scoresAuditory['test_precision'].mean())
print('Test Recall Auditory:', scoresAuditory['test_recall'], 'Mean:', scoresAuditory['test_recall'].mean())
print('Test F1 Auditory:', scoresAuditory['test_f1'], 'Mean:', scoresAuditory['test_f1'].mean())

# train and test a new model with the parameters found by grid search

X_train, X_test, y_train, y_test = train_test_split(X_Auditory, Y_Auditory, test_size=0.4, shuffle=False)

# create a KNN classifier according to the best hyperparameters found above
knn = KNeighborsClassifier(n_neighbors=8)
knn.fit(X_train, y_train)

# Predict on training and testing data
y_train_pred = knn.predict(X_train)
y_test_pred = knn.predict(X_test)

# Evaluate performance
train_accuracy = accuracy_score(y_train, y_train_pred)
test_accuracy = accuracy_score(y_test, y_test_pred)
test_precision = precision_score(y_test, y_test_pred, average='binary')
test_recall = recall_score(y_test, y_test_pred, average='binary')
test_f1 = f1_score(y_test, y_test_pred, average='binary')

# Print results
print("KNN on ERP data final model:")
print(f"Training Accuracy: {train_accuracy:.4f}")
print(f"Testing Accuracy: {test_accuracy:.4f}")
print(f"Testing Precision: {test_precision:.4f}")
print(f"Testing Recall: {test_recall:.4f}")
print(f"Testing F1 Score: {test_f1:.4f}")

#KNN Rest
# Define the range of k values to test
param_grid = {'n_neighbors': np.arange(1, 21)}

# Define the scoring metrics with zero_division parameter
scoring = {
    'accuracy': make_scorer(accuracy_score),
    'precision': make_scorer(precision_score, zero_division=0),
    'recall': make_scorer(recall_score, zero_division=0),
    'f1': make_scorer(f1_score, zero_division=0)
}

# Create the KNN classifier
knn = KNeighborsClassifier()

# Set up GridSearchCV with 5-fold cross-validation
cv = KFold(n_splits=5, shuffle=False)
grid_search = GridSearchCV(knn, param_grid, cv=cv, scoring=scoring, refit='accuracy', return_train_score=True)

# Perform the grid search on the rest data
grid_search.fit(X_Rest, Y_Rest)

# Get the best parameters and the corresponding scores
best_k = grid_search.best_params_['n_neighbors']
best_score = grid_search.best_score_

print(f"Best k: {best_k}")
print(f"Best cross-validated accuracy: {best_score}")

# Extract the scores for the best model
best_model = grid_search.best_estimator_
scoresRest = cross_validate(best_model, X_Rest, Y_Rest, cv=cv, scoring=scoring, return_train_score=True)

print("Metrics Rest with optimized k:")
print('Test Accuracy Rest:', scoresRest['test_accuracy'], 'Mean:', scoresRest['test_accuracy'].mean())
print('Train Accuracy Rest:', scoresRest['train_accuracy'], 'Mean:', scoresRest['train_accuracy'].mean())
print('Test Precision Rest:', scoresRest['test_precision'], 'Mean:', scoresRest['test_precision'].mean())
print('Test Recall Rest:', scoresRest['test_recall'], 'Mean:', scoresRest['test_recall'].mean())
print('Test F1 Rest:', scoresRest['test_f1'], 'Mean:', scoresRest['test_f1'].mean())

"""### Perform Feature selection

The classifier is obviously overfitting to the training data in each fold -- we need to do some sort of feature selection
"""

# select the best k features using ANOVA
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import f_classif

# loop over some different values for k

k_values = np.arange(1, 51)
test_acc = []
train_acc = []

for k in k_values:

  # define feature selection
  fs = SelectKBest(score_func=f_classif, k=k)
  # apply feature selection
  X_selected = fs.fit_transform(X_Rest, Y_Rest)
  #print(X_selected.shape)

  # train the model with only the k best features & perform 5-fold cross validation

  # Define the range of k values to test
  param_grid = {'n_neighbors': np.arange(1, 21)}

  # Define the scoring metrics with zero_division parameter
  scoring = {
      'accuracy': make_scorer(accuracy_score),
      'precision': make_scorer(precision_score, zero_division=0),
      'recall': make_scorer(recall_score, zero_division=0),
      'f1': make_scorer(f1_score, zero_division=0)
  }

  # Create the KNN classifier
  knn = KNeighborsClassifier()

  # Set up GridSearchCV with 5-fold cross-validation
  cv = KFold(n_splits=5, shuffle=False)
  grid_search = GridSearchCV(knn, param_grid, cv=cv, scoring=scoring, refit='accuracy', return_train_score=True)

  # Perform the grid search on the rest data
  grid_search.fit(X_selected, Y_Rest)

  # Get the best parameters and the corresponding scores
  best_k = grid_search.best_params_['n_neighbors']
  best_score = grid_search.best_score_

  #print(f"USING THE BEST k = {k} FEATURES:")

  #print(f"Best k: {best_k}")
  #print(f"Best cross-validated accuracy: {best_score}")

  # Extract the scores for the best model
  best_model = grid_search.best_estimator_
  scoresRest = cross_validate(best_model, X_selected, Y_Rest, cv=cv, scoring=scoring, return_train_score=True)

  #print("Metrics Rest with optimized k:")
  #print('Test Accuracy Rest:', scoresRest['test_accuracy'], 'Mean:', scoresRest['test_accuracy'].mean())
  #print('Train Accuracy Rest:', scoresRest['train_accuracy'], 'Mean:', scoresRest['train_accuracy'].mean())
  #print('Test Precision Rest:', scoresRest['test_precision'], 'Mean:', scoresRest['test_precision'].mean())
  #print('Test Recall Rest:', scoresRest['test_recall'], 'Mean:', scoresRest['test_recall'].mean())
  #print('Test F1 Rest:', scoresRest['test_f1'], 'Mean:', scoresRest['test_f1'].mean())

  test_acc.append(scoresRest['test_accuracy'].mean())
  train_acc.append(scoresRest['train_accuracy'].mean())

  #print(test_acc, train_acc)

import matplotlib.pyplot as plt

# Plotting the data
plt.figure(figsize=(10, 6))
plt.plot(k_values, test_acc, label='Test Accuracy', marker='o')
plt.plot(k_values, train_acc, label='Train Accuracy', marker='o')
plt.xlabel('Value for k')
plt.ylabel('Mean Accuracy')
plt.title('KNN Resting: \n Mean Accuracy Over 5 Folds Using k Best Features')
plt.legend()
plt.grid(True)
plt.show()

"""Now that we found the best k for SelectKBest, create a new dataframe with only those columns and run the algorithm on it again to get all performance measures."""

# get the list with the 3 best features

fs = SelectKBest(score_func=f_classif, k=3)
fs.fit_transform(X_Rest, Y_Rest)

mask = fs.get_support()
new_features = [] # list of the K best features

for bool_val, feature in zip(mask, X_Rest.columns):
    if bool_val:
        new_features.append(feature)

print(new_features)

X_Rest_selected = X_Rest[new_features]
display(X_Rest_selected)

# export to csv
X_Rest_selected.to_csv('KNN_X_resting_selected.csv')

#KNN Rest
# Define the range of k values to test
param_grid = {'n_neighbors': np.arange(1, 21)}

# Define the scoring metrics with zero_division parameter
scoring = {
    'accuracy': make_scorer(accuracy_score),
    'precision': make_scorer(precision_score, zero_division=0),
    'recall': make_scorer(recall_score, zero_division=0),
    'f1': make_scorer(f1_score, zero_division=0)
}

# Create the KNN classifier
knn = KNeighborsClassifier()

# Set up GridSearchCV with 5-fold cross-validation
cv = KFold(n_splits=5, shuffle=False)
grid_search = GridSearchCV(knn, param_grid, cv=cv, scoring=scoring, refit='accuracy', return_train_score=True)

# Perform the grid search on the rest data
grid_search.fit(X_Rest_selected, Y_Rest)

# Get the best parameters and the corresponding scores
best_k = grid_search.best_params_['n_neighbors']
best_score = grid_search.best_score_

print(f"Best k: {best_k}")
print(f"Best cross-validated accuracy: {best_score}")

# Extract the scores for the best model
best_model = grid_search.best_estimator_
scoresRest = cross_validate(best_model, X_Rest_selected, Y_Rest, cv=cv, scoring=scoring, return_train_score=True)

print("Metrics Rest selected with optimized k:")
print('Test Accuracy Rest:', scoresRest['test_accuracy'], 'Mean:', scoresRest['test_accuracy'].mean())
print('Train Accuracy Rest:', scoresRest['train_accuracy'], 'Mean:', scoresRest['train_accuracy'].mean())
print('Test Precision Rest:', scoresRest['test_precision'], 'Mean:', scoresRest['test_precision'].mean())
print('Test Recall Rest:', scoresRest['test_recall'], 'Mean:', scoresRest['test_recall'].mean())
print('Test F1 Rest:', scoresRest['test_f1'], 'Mean:', scoresRest['test_f1'].mean())

# train and test a new model with the parameters found by grid search
X_train, X_test, y_train, y_test = train_test_split(X_Rest_selected, Y_Rest, test_size=0.4, shuffle=False)

# create a KNN classifier according to the best hyperparameters found above
knn = KNeighborsClassifier(n_neighbors=17)
knn.fit(X_train, y_train)

# Predict on training and testing data
y_train_pred = knn.predict(X_train)
y_test_pred = knn.predict(X_test)

# Evaluate performance
train_accuracy = accuracy_score(y_train, y_train_pred)
test_accuracy = accuracy_score(y_test, y_test_pred)
test_precision = precision_score(y_test, y_test_pred, average='binary')
test_recall = recall_score(y_test, y_test_pred, average='binary')
test_f1 = f1_score(y_test, y_test_pred, average='binary')

# Print results
print("KNN on resting data final model:")
print(f"Training Accuracy: {train_accuracy:.4f}")
print(f"Testing Accuracy: {test_accuracy:.4f}")
print(f"Testing Precision: {test_precision:.4f}")
print(f"Testing Recall: {test_recall:.4f}")
print(f"Testing F1 Score: {test_f1:.4f}")